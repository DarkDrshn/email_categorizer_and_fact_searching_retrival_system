{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "DATA PREPROCESSING\n",
        "\n"
      ],
      "metadata": {
        "id": "31f0vQz13_3L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLpDk8HggBZx",
        "outputId": "1e1c2ef7-55c7-40b3-c29d-2eaf2eb5cd0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuibsbFQhy1k",
        "outputId": "4f90d818-c024-426a-8d7e-6cd21a480f22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145449\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "tweets = []\n",
        "for line in open(\"/content/drive/MyDrive/My Work/IR/train.jsonl\", 'r'):\n",
        "    tweets.append(json.loads(line))\n",
        "print(len(tweets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNzRR8beivbE",
        "outputId": "8f2929af-2c70-4837-b6c4-4b69f46653e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 75397, 'verifiable': 'VERIFIABLE', 'label': 'SUPPORTS', 'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'evidence': [[[92206, 104971, 'Nikolaj_Coster-Waldau', 7], [92206, 104971, 'Fox_Broadcasting_Company', 0]]]}\n"
          ]
        }
      ],
      "source": [
        "print(tweets[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLDpSA8Djwij",
        "outputId": "de02f138-840a-4564-b8f2-becc597dc49d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[92206, 104971, 'Nikolaj_Coster-Waldau', 7], [92206, 104971, 'Fox_Broadcasting_Company', 0]], 2]\n"
          ]
        }
      ],
      "source": [
        "evidence = []\n",
        "for t in tweets:\n",
        "  evi = []\n",
        "  if len(t['evidence'][0]) >1:\n",
        "    evi.append(t['evidence'][0])\n",
        "    evi.append(len(t['evidence'][0]))\n",
        "  else :\n",
        "    evi.append(t['evidence'][0][0])\n",
        "    if t['evidence'][0][0][2] is None:\n",
        "      evi.append(None)\n",
        "    else:\n",
        "      evi.append(len(t['evidence'][0]))\n",
        "  evidence.append(evi)\n",
        "print(evidence[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueCUdHGVlKkV"
      },
      "outputs": [],
      "source": [
        "verifiable_ids = []\n",
        "non_verifiable_ids = []\n",
        "for t in tweets:\n",
        "  if t[\"verifiable\"] == \"VERIFIABLE\":\n",
        "    verifiable_ids.append(t['id'])\n",
        "  else:\n",
        "    non_verifiable_ids.append(t['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXmsazqBlzDt"
      },
      "outputs": [],
      "source": [
        "with open('verifiable_ids.txt', 'w') as f:\n",
        "    for item in verifiable_ids:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "with open('non_verifiable_ids.txt', 'w') as f:\n",
        "    for item in non_verifiable_ids:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU3RQrvFngGs",
        "outputId": "c7256d98-3efd-4e95-d715-41822ff9952e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[151831, 166598, 'Homeland_-LRB-TV_series-RRB-', 0], [151831, 166598, 'Prisoners_of_War_-LRB-TV_series-RRB-', 0]], 2]\n"
          ]
        }
      ],
      "source": [
        "evidence = []\n",
        "claim = []\n",
        "label = []\n",
        "id = []\n",
        "for t in tweets:\n",
        "  evi = []\n",
        "  if(t['label']==\"NOT ENOUGH INFO\"):\n",
        "    continue\n",
        "  claim.append(t['claim'])\n",
        "  label.append(t['label'])\n",
        "  id.append(t['id'])\n",
        "  if len(t['evidence'][0]) >1:\n",
        "    evi.append(t['evidence'][0])\n",
        "    evi.append(len(t['evidence'][0]))\n",
        "  else :\n",
        "    evi.append(t['evidence'][0][0])\n",
        "    if t['evidence'][0][0][2] is None:\n",
        "      evi.append(None)\n",
        "    else:\n",
        "      evi.append(len(t['evidence'][0]))\n",
        "  evidence.append(evi)\n",
        "print(evidence[4])\n",
        "with open('evidence.txt', 'w') as f:\n",
        "    for item in evidence:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "with open('claim.txt', 'w') as f:\n",
        "    for item in claim:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "with open('label.txt', 'w') as f:\n",
        "    for item in label:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcT8PawVMCfB"
      },
      "outputs": [],
      "source": [
        "with open('id.txt', 'w') as f:\n",
        "    for item in id:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKumoEhd5X-w",
        "outputId": "d51aa815-612a-4458-bf7f-addb055d314c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109810\n",
            "109810\n",
            "109810\n",
            "109810\n"
          ]
        }
      ],
      "source": [
        "print(len(claim))\n",
        "print(len(evidence))\n",
        "print(len(label))\n",
        "print(len(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZOMFVv43Mcu",
        "outputId": "0c65defb-62d0-4560-c056-13ea4ffa1bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "1030609\n",
            "110058\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # For tokenizers\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "claim_file = open(\"claim.txt\").read()\n",
        "tokenizer = nltk.tokenize.punkt.PunktSentenceTokenizer()\n",
        "# tokenizer.tokenize(claim)\n",
        "sents = nltk.sent_tokenize(claim_file)\n",
        "words = nltk.word_tokenize(claim_file)\n",
        "print(len(words))\n",
        "print(len(sents))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqYsHRSO9G-x",
        "outputId": "a439d307-f92a-4e64-f62e-38512b7c6913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\n",
            "Roman Atwood is a content creator.\n",
            "History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.\n",
            "Adrienne Bailon is an accountant.\n",
            "Homeland is an American television spy thriller based on the Israeli television series Prisoners of War.\n",
            "['nikolaj', 'worked', 'with', 'the', 'fox']\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(sents[i])\n",
        "words=[word.lower() for word in words if word.isalpha()]\n",
        "print(words[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zajHZ3fI5z72",
        "outputId": "7423bd05-941c-482a-b224-31faad9fee72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d48PlM317H2B",
        "outputId": "7dca02bc-d4a2-4c3e-8749-b2ffe5d4bdc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average number of tokens per sentence:  8\n",
            "['Nikolaj', 'Coster-Waldau', 'worked', 'with', 'the', 'Fox', 'Broadcasting', 'Company', '.']\n",
            "109810\n",
            "['nikolaj', 'worked', 'fox', 'broadcasting', 'company']\n"
          ]
        }
      ],
      "source": [
        "print(\"The average number of tokens per sentence: \",round(len(words) / len(sents)))\n",
        "stop_words = set(stopwords.words('english'))\n",
        "sents1 = []\n",
        "ans = []\n",
        "\n",
        "for each in claim:\n",
        "  sents1.append(word_tokenize(each))\n",
        "\n",
        "print(sents1[0])\n",
        "print(len(sents1))\n",
        "sent_without_sw = [word.lower() for word in sents1[0] if not word in stopwords.words() and word.isalpha()]\n",
        "# sents2.append(sent_without_sw)\n",
        "print(sent_without_sw)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9HjJ-2K5y1"
      },
      "outputs": [],
      "source": [
        "sents2 = []\n",
        "for each in sents1:\n",
        "  sent_without_sw = [word.lower() for word in each if not word in stopwords.words() and word.isalpha()]\n",
        "  sents2.append(sent_without_sw)\n",
        "  # break\n",
        "print(sents2[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with open('claimsss.txt', 'w') as f:\n",
        "    for item in sents2:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "a5hoguiRjFXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('claimsss.txt', 'w') as f:\n",
        "    for item in sents2:\n",
        "        f.write(\"%s\\n\" % item)"
      ],
      "metadata": {
        "id": "cP4KrWcadLM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "fields=['id','claim','label','evidence']\n",
        "rows=[]\n",
        "blank=[]\n",
        "for i in range (len(tweets)):\n",
        "  val = id\n",
        "  val2=sents2\n",
        "  val3=label\n",
        "  val4=evidence\n",
        "  blank.append(val)\n",
        "  blank.append(val2)\n",
        "  blank.append(val3)\n",
        "  blank.append(val4)\n",
        "  rows.append(blank)\n",
        "  blank=[]"
      ],
      "metadata": {
        "id": "jynvj4h6yuTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"FEVER.csv\"\n",
        "\n",
        "# writing to csv file\n",
        "with open(filename, 'w') as csvfile:\n",
        "    # creating a csv writer object\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "\n",
        "    # writing the fields\n",
        "    csvwriter.writerow(fields)\n",
        "\n",
        "    # writing the data rows\n",
        "    csvwriter.writerows(rows)"
      ],
      "metadata": {
        "id": "tXpo1zDLBKYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKXRU4emlUel"
      },
      "outputs": [],
      "source": [
        "url_list = []\n",
        "for e in evidence:\n",
        "  if len(e)>1:\n",
        "    for l in e:\n",
        "      assert type(l[0][2]) == str\n",
        "      url_list.append(l[0][2])\n",
        "  else:\n",
        "    if type(e[0][0][2]) == str:\n",
        "      url_list.append(e[0][0][2])\n",
        "url_list = list(set(url_list))\n",
        "url_list.sort()\n",
        "print(url_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTXR__aFqtfo"
      },
      "outputs": [],
      "source": [
        "query_id = []\n",
        "for e in evidence:\n",
        "  query_id.append(e[0][0][0])\n",
        "print(query_id[0:100])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}